{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolayWTF/Building_a_GPT/blob/main/Esenin_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing"
      ],
      "metadata": {
        "id": "peQN8Wb6opKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1QSkQrVLv-sshOYlnPMMyh-ooGbAQ_pr4"
      ],
      "metadata": {
        "id": "AU_QMIxQo8Ep",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0a6c0d-257d-431a-c998-dee84ac6acda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QSkQrVLv-sshOYlnPMMyh-ooGbAQ_pr4\n",
            "To: /content/esenin.txt\n",
            "\r  0% 0.00/158k [00:00<?, ?B/s]\r100% 158k/158k [00:00<00:00, 5.04MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('esenin.txt', 'r', encoding='cp1251') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "hGyP6QZ-ooyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def is_date_line(line):\n",
        "    line = line.strip()\n",
        "    return (\n",
        "        re.match(r'^\\d{4}$', line) or                           # 1910\n",
        "        re.match(r'^<\\d{4}>$', line) or                         # <1925>\n",
        "        re.match(r'^<\\d{4}-\\d{4}>$', line) or                    # <1916-1922>\n",
        "        re.match(r'^<\\d{4}\\?>$', line) or                         # <1917?>\n",
        "        re.match(r'^\\d{1,2}(\\/\\d{1,2})?\\s+[а-яё]+\\s+\\d{4}$', line, re.IGNORECASE) or  # 4/5 октября 1925, 3 октября 1925\n",
        "        re.match(r'^[а-яё]+\\s+\\d{4}$', line, re.IGNORECASE) or    # Февраль 1922\n",
        "        False\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_poems(text):\n",
        "    special_words = [\"&#8196;\", \"[Другая редакция]\", \"[Вторая редакция]\",\n",
        "                     \"[Первая редакция]\", \"[Окончательная редакция]\",\n",
        "                     \"&#769;\", \"&#903;\"]\n",
        "    for special_word in special_words:\n",
        "        text = text.replace(special_word, '')\n",
        "\n",
        "    # Разбиваем по заголовкам в кавычках\n",
        "    raw_poems = re.findall(r'\"(.*?)\"\\s+((?:.|\\n)*?)(?=\\n+\"\\s*|$)', text)\n",
        "\n",
        "    poems = []\n",
        "    for title, body in raw_poems:\n",
        "        # Удалим звёздочку и повторение заголовка\n",
        "        lines = body.strip().split('\\n')\n",
        "        clean_lines = []\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            # Убираем звёздочку и пустые строки\n",
        "            if not line or '*' in line or is_date_line(line):\n",
        "                continue\n",
        "            clean_lines.append(line)\n",
        "\n",
        "        poem_text = '\\n'.join(clean_lines)\n",
        "        poems.append({\n",
        "            'title': title.strip(),\n",
        "            'body': poem_text.strip()\n",
        "        })\n",
        "\n",
        "    return poems\n"
      ],
      "metadata": {
        "id": "IiDIRQrpouBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poems = preprocess_poems(text)"
      ],
      "metadata": {
        "id": "Q4dTjinNowSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = \"\"\n",
        "for i in range(len(poems)):\n",
        "  title = poems[i]['title'].replace(\"...\", \"\").replace(\"\\n\", \"\")\n",
        "  first_row = poems[i]['body'].split(\"\\n\")[0].replace(\".\", \"\").replace(\",\", \"\").replace(\"\\n\", \"\")\n",
        "\n",
        "  if title.replace(\",\", \"\") == first_row:\n",
        "      final_text += poems[i]['body'] + \"\\n\" + \"\\n\"\n",
        "  else:\n",
        "      final_text += title + \"\\n\" + poems[i]['body'] + \"\\n\" + \"\\n\""
      ],
      "metadata": {
        "id": "czYR7B0boynI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"esenin_preprocessed.txt\", \"w\")\n",
        "file.write(final_text)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "IL76156_o1_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('esenin_preprocessed.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "b32e03a6-c0be-47f4-ceb9-9d969791b00d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество символов в датасете:  103263\n"
          ]
        }
      ],
      "source": [
        "print(\"Количество символов в датасете: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "7ceddd36-23aa-4161-87d6-f980f7b62d37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вот уж вечер. Роса\n",
            "Блестит на крапиве.\n",
            "Я стою у дороги,\n",
            "Прислонившись к иве.\n",
            "От луны свет большой\n",
            "Прямо на нашу крышу.\n",
            "Где-то песнь соловья\n",
            "Вдалеке я слышу.\n",
            "Хорошо и тепло,\n",
            "Как зимой у печки.\n",
            "И березы стоят,\n",
            "Как большие свечки.\n",
            "И вдали за рекой,\n",
            "Видно, за опушкой,\n",
            "Сонный сторож стучит\n",
            "Мертвой колотушкой.\n",
            "\n",
            "Там, где капустные грядки\n",
            "Красной водой поливает восход,\n",
            "Клененочек маленький матке\n",
            "Зеленое вымя сосет.\n",
            "\n",
            "Поет зима - аукает\n",
            "Мохнатый лес баюкает\n",
            "Стозвоном сосняка.\n",
            "Кругом с тоской глубокою\n",
            "Плывут в страну далекую\n",
            "Седые облака.\n",
            "А по двору метелица\n",
            "Ковром шелковым стелется,\n",
            "Но больно холодна.\n",
            "Воробышки игривые,\n",
            "Как детки сиротливые,\n",
            "Прижались у окна.\n",
            "Озябли пташки малые,\n",
            "Голодные, усталые,\n",
            "И жмутся поплотней.\n",
            "А вьюга с ревом бешеным\n",
            "Стучит по ставням свешенным\n",
            "И злится все сильней.\n",
            "И дремлют пташки нежные\n",
            "Под эти вихри снежные\n",
            "У мерзлого окна.\n",
            "И снится им прекрасная,\n",
            "В улыбках солнца ясная\n",
            "Красавица весна.\n",
            "\n",
            "Под венком лесной ромашки\n",
            "Я строгал, чинил челны,\n",
            "Уронил кольцо милашки\n",
            "В струи \n"
          ]
        }
      ],
      "source": [
        "# Выведем первые 1000 символов\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "fa307633-8c24-4dd4-97d3-a660d69e7524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\",-.:;?АБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё\n",
            "71\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text))) # Отсортированый список всех уникальных символов в тексте (словарь)\n",
        "vocab_size = len(chars) # Размер словаря\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "d7c28624-6426-4b0a-8f09-340cb5d77677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[24, 54, 46, 40, 43, 56, 1, 21, 46, 54, 1, 37, 1, 15, 55, 43, 51, 46, 51]\n",
            "Привет Мир Я Есенин\n"
          ]
        }
      ],
      "source": [
        "# Все уникальные символы, которые встречаются в этом тексте\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # Пример: если chars = ['a', 'b', 'c'], то stoi = { 'a': 0, 'b': 1, 'c': 2 }\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # Такой же словарь, как и stoi, только ключ и значение изменены местами\n",
        "encode = lambda s: [stoi[c] for c in s] # Функция, переводящая строку в список чисел в соответствии с stoi (encoder)\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Функция, переводящая список чисел в строку соответствии с itos (decoder)\n",
        "\n",
        "example = \"Привет Мир Я Есенин\"\n",
        "ex_encode = encode(example)\n",
        "ex_decode = decode(ex_encode)\n",
        "\n",
        "print(ex_encode)\n",
        "print(ex_decode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "fc938c80-11fd-4258-9b6d-7cf95d93b498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([103263]) torch.int64\n",
            "tensor([12, 52, 56,  1, 57, 44,  1, 40, 43, 61, 43, 54,  6,  1, 25, 52, 55, 38,\n",
            "         0, 11, 49, 43, 55, 56, 46, 56,  1, 51, 38,  1, 48, 54, 38, 53, 46, 40,\n",
            "        43,  6,  0, 37,  1, 55, 56, 52, 68,  1, 57,  1, 42, 52, 54, 52, 41, 46,\n",
            "         4,  0, 24, 54, 46, 55, 49, 52, 51, 46, 40, 62, 46, 55, 66,  1, 48,  1,\n",
            "        46, 40, 43,  6,  0, 23, 56,  1, 49, 57, 51, 65,  1, 55, 40, 43, 56,  1,\n",
            "        39, 52, 49, 66, 62, 52, 47,  0, 24, 54])\n"
          ]
        }
      ],
      "source": [
        "# Теперь закодируем весь текстовый датасет и сохраним его в torch.Tensor — это основной тип данных в PyTorch\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "\n",
        "# Преобразует весь текст в список чисел, используя encode из прошлой ячейки\n",
        "# превращает этот список чисел в тензор PyTorch с целочисленным типом (long, 64-битное целое число).\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100]) # the 100 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Сейчас разделим весь текст (в виде тензора data) на обучающую (train) и валидационную (validation) выборки.\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "2231a3ba-28a4-4cca-b7e7-92aa71fbdd50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12, 52, 56,  1, 57, 44,  1, 40, 43])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "# Задаем размер контекста (то есть, сколько символов модель \"видит\" одновременно).\n",
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "c1635b78-c3b7-47e0-a144-78ac8f25ccd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Если контекст = tensor([12]) нужно предсказать: 52\n",
            "Если контекст = tensor([12, 52]) нужно предсказать: 56\n",
            "Если контекст = tensor([12, 52, 56]) нужно предсказать: 1\n",
            "Если контекст = tensor([12, 52, 56,  1]) нужно предсказать: 57\n",
            "Если контекст = tensor([12, 52, 56,  1, 57]) нужно предсказать: 44\n",
            "Если контекст = tensor([12, 52, 56,  1, 57, 44]) нужно предсказать: 1\n",
            "Если контекст = tensor([12, 52, 56,  1, 57, 44,  1]) нужно предсказать: 40\n",
            "Если контекст = tensor([12, 52, 56,  1, 57, 44,  1, 40]) нужно предсказать: 43\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size] # первые 8 токенов — входные данные.\n",
        "y = train_data[1:block_size+1] # те же токены, но сдвинуты на один вперед — цели (что должно быть предсказано).\n",
        "\n",
        "# Цикл, в котором выводится информация какой токен должна предсказать модель\n",
        "# при подаче определённого контекста\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"Если контекст = {context} нужно предсказать: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "bdf27d71-1750-40f2-cef6-628dc2c2ff53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 0, 32, 57, 56, 66,  1, 45, 38],\n",
            "        [44, 46, 45, 51, 66,  1, 53, 54],\n",
            "        [51, 51, 65, 59,  6,  0, 24, 52],\n",
            "        [51, 52, 47,  1, 46,  1, 55, 52]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[32, 57, 56, 66,  1, 45, 38, 39],\n",
            "        [46, 45, 51, 66,  1, 53, 54, 52],\n",
            "        [51, 65, 59,  6,  0, 24, 52,  1],\n",
            "        [52, 47,  1, 46,  1, 55, 52, 49]])\n",
            "----\n",
            "when input is [0] the target: 32\n",
            "when input is [0, 32] the target: 57\n",
            "when input is [0, 32, 57] the target: 56\n",
            "when input is [0, 32, 57, 56] the target: 66\n",
            "when input is [0, 32, 57, 56, 66] the target: 1\n",
            "when input is [0, 32, 57, 56, 66, 1] the target: 45\n",
            "when input is [0, 32, 57, 56, 66, 1, 45] the target: 38\n",
            "when input is [0, 32, 57, 56, 66, 1, 45, 38] the target: 39\n",
            "when input is [44] the target: 46\n",
            "when input is [44, 46] the target: 45\n",
            "when input is [44, 46, 45] the target: 51\n",
            "when input is [44, 46, 45, 51] the target: 66\n",
            "when input is [44, 46, 45, 51, 66] the target: 1\n",
            "when input is [44, 46, 45, 51, 66, 1] the target: 53\n",
            "when input is [44, 46, 45, 51, 66, 1, 53] the target: 54\n",
            "when input is [44, 46, 45, 51, 66, 1, 53, 54] the target: 52\n",
            "when input is [51] the target: 51\n",
            "when input is [51, 51] the target: 65\n",
            "when input is [51, 51, 65] the target: 59\n",
            "when input is [51, 51, 65, 59] the target: 6\n",
            "when input is [51, 51, 65, 59, 6] the target: 0\n",
            "when input is [51, 51, 65, 59, 6, 0] the target: 24\n",
            "when input is [51, 51, 65, 59, 6, 0, 24] the target: 52\n",
            "when input is [51, 51, 65, 59, 6, 0, 24, 52] the target: 1\n",
            "when input is [51] the target: 52\n",
            "when input is [51, 52] the target: 47\n",
            "when input is [51, 52, 47] the target: 1\n",
            "when input is [51, 52, 47, 1] the target: 46\n",
            "when input is [51, 52, 47, 1, 46] the target: 1\n",
            "when input is [51, 52, 47, 1, 46, 1] the target: 55\n",
            "when input is [51, 52, 47, 1, 46, 1, 55] the target: 52\n",
            "when input is [51, 52, 47, 1, 46, 1, 55, 52] the target: 49\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337) # То же самое, что random(seed=42)\n",
        "\n",
        "batch_size = 4 # модель будет обучаться сразу на batch_size независимых примерах параллельно.\n",
        "block_size = 8 # максимальная длина входной последовательности (контекста) для предсказания следующего токена.\n",
        "\n",
        "def get_batch(split: str):\n",
        "    \"\"\"\n",
        "     Функция создает один обучающий/валидационный батч.\n",
        "     Она вернёт 2 тензора: входы x и цели y\n",
        "\n",
        "     split - строка, говорящая с какой выборкой работаем\n",
        "     \"\"\"\n",
        "    # трейн или вал?\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Генерируем batch_size стартовых позиций. Стартовая позиция - рандомное\n",
        "    # число от 0 до len(data) - block_size\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "\n",
        "    # Для всех выбраных значений из ix, берём ещё block_size-1 следующих токенов\n",
        "    # В итоге получим batch_size(4) контекста\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    # целевая переменная для этих контекстов - последовательность токенов,\n",
        "    # сдвинутых на 1 вправо\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train') # Получаем тренировочную выборку (1 батч)\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # С каким набором токенов из батча работаем\n",
        "    for t in range(block_size): # Изменяем t от 0 до block_size-1\n",
        "        context = xb[b, :t+1] # Берём контекст длинной t+1 из списка токенов\n",
        "        target = yb[b,t] # Целевая переменная лежит в y под индексом t\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "e2844ece-8ca0-428f-eb54-5f30f990e2e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0, 32, 57, 56, 66,  1, 45, 38],\n",
            "        [44, 46, 45, 51, 66,  1, 53, 54],\n",
            "        [51, 51, 65, 59,  6,  0, 24, 52],\n",
            "        [51, 52, 47,  1, 46,  1, 55, 52]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HH6FF0FDdayC",
        "outputId": "8064f0e7-4d6e-46dd-a5bb-17b4b65d2927"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[32, 57, 56, 66,  1, 45, 38, 39],\n",
              "        [46, 45, 51, 66,  1, 53, 54, 52],\n",
              "        [51, 65, 59,  6,  0, 24, 52,  1],\n",
              "        [52, 47,  1, 46,  1, 55, 52, 49]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ],
      "source": [
        "yb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "149f31e9-f813-4ea7-9e61-4b40fdca0c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 71])\n",
            "tensor(4.9440, grad_fn=<NllLossBackward0>)\n",
            "\n",
            ".ъйгяупЖаН,бэблПухае\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module): # Создаём подкласс от nn.Module, который реализует биграммную модель.\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        \"\"\"\n",
        "        vocab_size - размер словаря. Количество уникальных токенов.\n",
        "\n",
        "        nn.Embedding(vocab_size, vocab_size) — матрица весов (размером vocab_size x vocab_size).\n",
        "        Каждому токену ставится в соответствие вектор длины vocab_size — это будет логитами (сырые предсказания) следующего токена.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        \"\"\"\n",
        "        idx: вход — батч индексов токенов (размером [B, T], где B — размер батча, T — длина последовательности).\n",
        "\n",
        "        targets: правильные ответы, такие же по размеру.\n",
        "\n",
        "        Разбор на примере:\n",
        "          Если idx = tensor([1, 3, 4],\n",
        "                            [5, 7, 8]). Размер батча=2, длина последовательности=3\n",
        "          Тогда мы из таблицы self.token_embedding_table получаем 6 векторов длинной С (vocab_size)\n",
        "          6 векторов, потому что в idx всего 6 токенов (1, 3, 4, 5, 7, 8)\n",
        "\n",
        "          В итоге получаем логгиты размерностью [B, T, C]\n",
        "\n",
        "        Возвращает логгиты и loss\n",
        "        \"\"\"\n",
        "        # Получаем логиты (не вероятности!) следующего токена. Размер [B, T, C], где C = vocab_size.\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape # Получаем размерности логитов\n",
        "\n",
        "            # Это ресайз (view): превращаем 3D тензор [B, T, C] в 2D тензор [B*T, C].\n",
        "            # Зачем? Потому что F.cross_entropy ожидает логиты размера [N, C] (где N — количество примеров, а C — число классов).\n",
        "            logits = logits.view(B*T, C)\n",
        "            # Аналогично: приводим targets к размеру [B*T], чтобы был вектор меток на каждый токен.\n",
        "            targets = targets.view(B*T)\n",
        "\n",
        "            # F.cross_entropy ожидает:\n",
        "            # logits: тензор размера [N, C] — где каждая строка — логиты по всем классам,\n",
        "            # targets: вектор из N целых чисел — индексы правильных классов,\n",
        "            # делает внутри softmax, и считает средний log-loss (негативный логарифм вероятности правильного класса).\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "\n",
        "        # Генерирую max_new_tokens новых токенов\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Получаем логгиты (loss=Nan)\n",
        "            logits, loss = self.forward(idx)\n",
        "            # Для каждого набора токенов (B) берём только последний токен и его логгиты\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # Делаем softmax, получаем вероятности\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "            # Получаем следующий токен случайно из распределения, с учётом вероятностей. Делаем так для каждой последовательности в батче.\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "            # Приклеиваем новый токен idx_next к текущей последовательности idx по оси временной длины.\n",
        "            # Теперь idx содержит на один токен больше, и будет использоваться на следующей итерации.\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=20)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# Создаём оптимизатор — объект, который будет обновлять веса модели на каждом шаге обучения на основе градиентов.\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "63592562-3a75-4ad5-87bf-8a4355c24e5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.9145941734313965\n",
            "4.924348831176758\n",
            "4.777197360992432\n",
            "4.863997459411621\n",
            "5.00377893447876\n",
            "4.829232215881348\n",
            "4.815938949584961\n",
            "4.881904602050781\n",
            "4.844963073730469\n",
            "4.836766719818115\n",
            "4.804779529571533\n",
            "4.808426856994629\n",
            "4.8528828620910645\n",
            "4.885740280151367\n",
            "4.8669538497924805\n",
            "4.787975788116455\n",
            "4.975598335266113\n",
            "4.835967063903809\n",
            "4.809415340423584\n",
            "4.8151750564575195\n",
            "4.898438930511475\n",
            "4.851219177246094\n",
            "4.801431179046631\n",
            "4.932313442230225\n",
            "4.722620964050293\n",
            "4.914602756500244\n",
            "4.821335315704346\n",
            "4.7873406410217285\n",
            "4.826719760894775\n",
            "4.814449787139893\n",
            "4.674105167388916\n",
            "4.816755294799805\n",
            "4.902890682220459\n",
            "4.825515270233154\n",
            "4.827481269836426\n",
            "4.827781677246094\n",
            "4.760001182556152\n",
            "4.816375255584717\n",
            "4.847416400909424\n",
            "4.788262367248535\n",
            "4.789620876312256\n",
            "4.773386001586914\n",
            "4.759731292724609\n",
            "4.783736228942871\n",
            "4.861103057861328\n",
            "4.8432464599609375\n",
            "4.84573221206665\n",
            "4.765944480895996\n",
            "4.82210111618042\n",
            "4.894807815551758\n",
            "4.640679359436035\n",
            "4.768965721130371\n",
            "4.716592788696289\n",
            "4.824619293212891\n",
            "4.769155979156494\n",
            "4.828216075897217\n",
            "4.837564468383789\n",
            "4.812912940979004\n",
            "4.867977619171143\n",
            "4.780278205871582\n",
            "4.72634744644165\n",
            "4.75831413269043\n",
            "4.688039302825928\n",
            "4.861647605895996\n",
            "4.82904577255249\n",
            "4.785537242889404\n",
            "4.7657270431518555\n",
            "4.706006050109863\n",
            "4.783562183380127\n",
            "4.773908615112305\n",
            "4.763250350952148\n",
            "4.679938793182373\n",
            "4.778614044189453\n",
            "4.684538841247559\n",
            "4.79632568359375\n",
            "4.814486503601074\n",
            "4.734991550445557\n",
            "4.651815891265869\n",
            "4.774645805358887\n",
            "4.735757350921631\n",
            "4.793385028839111\n",
            "4.772889137268066\n",
            "4.860230445861816\n",
            "4.696559906005859\n",
            "4.827040195465088\n",
            "4.753636360168457\n",
            "4.774289608001709\n",
            "4.826716423034668\n",
            "4.768252372741699\n",
            "4.727819919586182\n",
            "4.751295566558838\n",
            "4.718822002410889\n",
            "4.79300594329834\n",
            "4.84781551361084\n",
            "4.891837120056152\n",
            "4.740565299987793\n",
            "4.718576431274414\n",
            "4.802336692810059\n",
            "4.873037338256836\n",
            "4.6604413986206055\n",
            "4.7086873054504395\n",
            "4.84672212600708\n",
            "4.65632438659668\n",
            "4.874513149261475\n",
            "4.753237724304199\n",
            "4.732252597808838\n",
            "4.737286567687988\n",
            "4.773200035095215\n",
            "4.691479682922363\n",
            "4.676684379577637\n",
            "4.691653728485107\n",
            "4.845930576324463\n",
            "4.733983039855957\n",
            "4.711108207702637\n",
            "4.7711992263793945\n",
            "4.628227710723877\n",
            "4.752806663513184\n",
            "4.784741401672363\n",
            "4.746133804321289\n",
            "4.694511890411377\n",
            "4.692864894866943\n",
            "4.7064948081970215\n",
            "4.595047950744629\n",
            "4.676320552825928\n",
            "4.593589782714844\n",
            "4.566815376281738\n",
            "4.801405429840088\n",
            "4.771857738494873\n",
            "4.715730667114258\n",
            "4.6952223777771\n",
            "4.66576623916626\n",
            "4.731231212615967\n",
            "4.646069526672363\n",
            "4.7703776359558105\n",
            "4.723695755004883\n",
            "4.723850250244141\n",
            "4.687680244445801\n",
            "4.616140842437744\n",
            "4.682163715362549\n",
            "4.721934795379639\n",
            "4.708517074584961\n",
            "4.751471042633057\n",
            "4.700235843658447\n",
            "4.71640157699585\n",
            "4.739110469818115\n",
            "4.6703925132751465\n",
            "4.6016435623168945\n",
            "4.663036346435547\n",
            "4.645867824554443\n",
            "4.689751148223877\n",
            "4.558083534240723\n",
            "4.765181541442871\n",
            "4.6483612060546875\n",
            "4.700002670288086\n",
            "4.674386024475098\n",
            "4.5602335929870605\n",
            "4.696954727172852\n",
            "4.617153644561768\n",
            "4.602319717407227\n",
            "4.658370018005371\n",
            "4.604611873626709\n",
            "4.661243915557861\n",
            "4.63052225112915\n",
            "4.742398262023926\n",
            "4.709356307983398\n",
            "4.644721984863281\n",
            "4.584982872009277\n",
            "4.653565406799316\n",
            "4.665915489196777\n",
            "4.643736362457275\n",
            "4.666093349456787\n",
            "4.554490566253662\n",
            "4.7194013595581055\n",
            "4.694974899291992\n",
            "4.648872375488281\n",
            "4.5846052169799805\n",
            "4.598905563354492\n",
            "4.6545329093933105\n",
            "4.670599937438965\n",
            "4.675840854644775\n",
            "4.66897439956665\n",
            "4.572956085205078\n",
            "4.623417377471924\n",
            "4.6283955574035645\n",
            "4.622329235076904\n",
            "4.572909355163574\n",
            "4.643945693969727\n",
            "4.657776832580566\n",
            "4.576121807098389\n",
            "4.667344570159912\n",
            "4.627785682678223\n",
            "4.680846214294434\n",
            "4.564864635467529\n",
            "4.576200485229492\n",
            "4.606961727142334\n",
            "4.713031768798828\n",
            "4.50078821182251\n",
            "4.604428291320801\n",
            "4.586906433105469\n",
            "4.716797828674316\n",
            "4.6079511642456055\n",
            "4.574932098388672\n",
            "4.617900371551514\n",
            "4.6441192626953125\n",
            "4.600037097930908\n",
            "4.666042804718018\n",
            "4.693709850311279\n",
            "4.654112815856934\n",
            "4.652079105377197\n",
            "4.6196699142456055\n",
            "4.561649322509766\n",
            "4.624753952026367\n",
            "4.58778190612793\n",
            "4.6155104637146\n",
            "4.639599323272705\n",
            "4.626692295074463\n",
            "4.588329792022705\n",
            "4.481812477111816\n",
            "4.6612162590026855\n",
            "4.624576091766357\n",
            "4.607464790344238\n",
            "4.555657863616943\n",
            "4.577556610107422\n",
            "4.64066219329834\n",
            "4.563993453979492\n",
            "4.59909725189209\n",
            "4.573829174041748\n",
            "4.5528178215026855\n",
            "4.5741143226623535\n",
            "4.566386699676514\n",
            "4.599575996398926\n",
            "4.597058296203613\n",
            "4.518215179443359\n",
            "4.56995964050293\n",
            "4.598431587219238\n",
            "4.44743013381958\n",
            "4.548550605773926\n",
            "4.518258571624756\n",
            "4.581627368927002\n",
            "4.573345184326172\n",
            "4.591018199920654\n",
            "4.547159194946289\n",
            "4.599002838134766\n",
            "4.619718074798584\n",
            "4.537293910980225\n",
            "4.543773651123047\n",
            "4.618371963500977\n",
            "4.625891208648682\n",
            "4.485230445861816\n",
            "4.565118789672852\n",
            "4.50824499130249\n",
            "4.613749027252197\n",
            "4.576563835144043\n",
            "4.576409816741943\n",
            "4.601797580718994\n",
            "4.460050582885742\n",
            "4.567356586456299\n",
            "4.658028602600098\n",
            "4.535615921020508\n",
            "4.512593746185303\n",
            "4.589027404785156\n",
            "4.582151889801025\n",
            "4.619625091552734\n",
            "4.4871506690979\n",
            "4.585224151611328\n",
            "4.435966491699219\n",
            "4.605551242828369\n",
            "4.571000099182129\n",
            "4.454395771026611\n",
            "4.553432464599609\n",
            "4.399582862854004\n",
            "4.583436965942383\n",
            "4.520252227783203\n",
            "4.507023811340332\n",
            "4.550363063812256\n",
            "4.56593656539917\n",
            "4.53810453414917\n",
            "4.529399871826172\n",
            "4.6047868728637695\n",
            "4.416184902191162\n",
            "4.487167835235596\n",
            "4.503943920135498\n",
            "4.559948921203613\n",
            "4.5743021965026855\n",
            "4.512859344482422\n",
            "4.554923057556152\n",
            "4.547719955444336\n",
            "4.598201751708984\n",
            "4.52056360244751\n",
            "4.517263412475586\n",
            "4.53811502456665\n",
            "4.519832134246826\n",
            "4.510169506072998\n",
            "4.460303783416748\n",
            "4.502475738525391\n",
            "4.468564033508301\n",
            "4.61765193939209\n",
            "4.530379295349121\n",
            "4.462208271026611\n",
            "4.583670616149902\n",
            "4.554978847503662\n",
            "4.499867916107178\n",
            "4.5270915031433105\n",
            "4.409786701202393\n",
            "4.44873571395874\n",
            "4.494004726409912\n",
            "4.533472061157227\n",
            "4.443635940551758\n",
            "4.428196907043457\n",
            "4.502589225769043\n",
            "4.511366844177246\n",
            "4.515733242034912\n",
            "4.551276206970215\n",
            "4.502288341522217\n",
            "4.539070129394531\n",
            "4.587823867797852\n",
            "4.520271301269531\n",
            "4.505361080169678\n",
            "4.468451499938965\n",
            "4.4332594871521\n",
            "4.472134590148926\n",
            "4.542430400848389\n",
            "4.562516212463379\n",
            "4.369445323944092\n",
            "4.450545310974121\n",
            "4.451193332672119\n",
            "4.475290298461914\n",
            "4.493074417114258\n",
            "4.472029685974121\n",
            "4.47634744644165\n",
            "4.482093334197998\n",
            "4.466604232788086\n",
            "4.4729132652282715\n",
            "4.501039981842041\n",
            "4.526147365570068\n",
            "4.487669467926025\n",
            "4.440150260925293\n",
            "4.452364921569824\n",
            "4.425786018371582\n",
            "4.5662031173706055\n",
            "4.567861557006836\n",
            "4.449419975280762\n",
            "4.4056620597839355\n",
            "4.434299468994141\n",
            "4.450947284698486\n",
            "4.374202728271484\n",
            "4.557273864746094\n",
            "4.396632671356201\n",
            "4.580015659332275\n",
            "4.457202911376953\n",
            "4.5144853591918945\n",
            "4.425037384033203\n",
            "4.477589130401611\n",
            "4.433172225952148\n",
            "4.530653953552246\n",
            "4.423794269561768\n",
            "4.4571452140808105\n",
            "4.463042736053467\n",
            "4.32560920715332\n",
            "4.465951442718506\n",
            "4.533230304718018\n",
            "4.43859338760376\n",
            "4.51032018661499\n",
            "4.543247222900391\n",
            "4.386967182159424\n",
            "4.455552101135254\n",
            "4.4274444580078125\n",
            "4.330338478088379\n",
            "4.345973491668701\n",
            "4.472824573516846\n",
            "4.520164966583252\n",
            "4.444726943969727\n",
            "4.356049060821533\n",
            "4.4663872718811035\n",
            "4.50138521194458\n",
            "4.469911575317383\n",
            "4.452587604522705\n",
            "4.38999080657959\n",
            "4.500257968902588\n",
            "4.4944562911987305\n",
            "4.412878036499023\n",
            "4.461280822753906\n",
            "4.387720108032227\n",
            "4.432363033294678\n",
            "4.380154132843018\n",
            "4.367927551269531\n",
            "4.435682773590088\n",
            "4.351414680480957\n",
            "4.450717449188232\n",
            "4.40096378326416\n",
            "4.503323078155518\n",
            "4.388995170593262\n",
            "4.384809494018555\n",
            "4.483180046081543\n",
            "4.363484859466553\n",
            "4.356640338897705\n",
            "4.4161152839660645\n",
            "4.366518020629883\n",
            "4.418579578399658\n",
            "4.449944496154785\n",
            "4.483335971832275\n",
            "4.344420909881592\n",
            "4.402405738830566\n",
            "4.5336594581604\n",
            "4.323263168334961\n",
            "4.388078689575195\n",
            "4.3796162605285645\n",
            "4.3525261878967285\n",
            "4.414548397064209\n",
            "4.431563854217529\n",
            "4.407591819763184\n",
            "4.388154029846191\n",
            "4.387801647186279\n",
            "4.360156536102295\n",
            "4.31981086730957\n",
            "4.362769603729248\n",
            "4.410158634185791\n",
            "4.437108993530273\n",
            "4.310694694519043\n",
            "4.3426337242126465\n",
            "4.305412769317627\n",
            "4.414313316345215\n",
            "4.278406620025635\n",
            "4.521971702575684\n",
            "4.4358625411987305\n",
            "4.321540832519531\n",
            "4.287804126739502\n",
            "4.324407577514648\n",
            "4.410861492156982\n",
            "4.249127388000488\n",
            "4.389997482299805\n",
            "4.436580181121826\n",
            "4.381558418273926\n",
            "4.4861931800842285\n",
            "4.463088035583496\n",
            "4.3482842445373535\n",
            "4.346938133239746\n",
            "4.444911479949951\n",
            "4.375313758850098\n",
            "4.293957710266113\n",
            "4.43770170211792\n",
            "4.388244152069092\n",
            "4.301175117492676\n",
            "4.344817638397217\n",
            "4.257662773132324\n",
            "4.359315872192383\n",
            "4.407354354858398\n",
            "4.290014266967773\n",
            "4.344610214233398\n",
            "4.382684230804443\n",
            "4.389622688293457\n",
            "4.3538713455200195\n",
            "4.313523769378662\n",
            "4.304557800292969\n",
            "4.437196731567383\n",
            "4.390214443206787\n",
            "4.328830242156982\n",
            "4.323731422424316\n",
            "4.407412052154541\n",
            "4.256923675537109\n",
            "4.349569320678711\n",
            "4.3546319007873535\n",
            "4.413991928100586\n",
            "4.250861644744873\n",
            "4.336301803588867\n",
            "4.364926815032959\n",
            "4.335841178894043\n",
            "4.2874836921691895\n",
            "4.286136627197266\n",
            "4.329031944274902\n",
            "4.373725414276123\n",
            "4.283146858215332\n",
            "4.280167579650879\n",
            "4.404870510101318\n",
            "4.325199127197266\n",
            "4.287744045257568\n",
            "4.407418251037598\n",
            "4.3944091796875\n",
            "4.376373767852783\n",
            "4.378653526306152\n",
            "4.222163677215576\n",
            "4.4435133934021\n",
            "4.346242904663086\n",
            "4.2736005783081055\n",
            "4.35587739944458\n",
            "4.254180908203125\n",
            "4.313940525054932\n",
            "4.317322254180908\n",
            "4.247496128082275\n",
            "4.3306379318237305\n",
            "4.288890838623047\n",
            "4.394454479217529\n",
            "4.363656044006348\n",
            "4.263760566711426\n",
            "4.322812080383301\n",
            "4.3452558517456055\n",
            "4.237903118133545\n",
            "4.320004463195801\n",
            "4.301366329193115\n",
            "4.308133125305176\n",
            "4.29193115234375\n",
            "4.158794403076172\n",
            "4.230731010437012\n",
            "4.255640029907227\n",
            "4.247252464294434\n",
            "4.362682819366455\n",
            "4.30476713180542\n",
            "4.322720050811768\n",
            "4.168568134307861\n",
            "4.264273643493652\n",
            "4.285285949707031\n",
            "4.341521739959717\n",
            "4.282325744628906\n",
            "4.294933319091797\n",
            "4.30505895614624\n",
            "4.315712928771973\n",
            "4.397840976715088\n",
            "4.31400203704834\n",
            "4.2015700340271\n",
            "4.2258620262146\n",
            "4.277847766876221\n",
            "4.250990867614746\n",
            "4.274866580963135\n",
            "4.236538887023926\n",
            "4.301666259765625\n",
            "4.147824287414551\n",
            "4.276845455169678\n",
            "4.318846225738525\n",
            "4.205927848815918\n",
            "4.252795696258545\n",
            "4.263753414154053\n",
            "4.220981121063232\n",
            "4.292069435119629\n",
            "4.301463603973389\n",
            "4.248297691345215\n",
            "4.170093059539795\n",
            "4.398038387298584\n",
            "4.292505264282227\n",
            "4.278288841247559\n",
            "4.284404277801514\n",
            "4.231186866760254\n",
            "4.275169372558594\n",
            "4.241981029510498\n",
            "4.23223352432251\n",
            "4.197551250457764\n",
            "4.2562689781188965\n",
            "4.275847434997559\n",
            "4.123422145843506\n",
            "4.220415115356445\n",
            "4.251835823059082\n",
            "4.232493877410889\n",
            "4.259655475616455\n",
            "4.20222282409668\n",
            "4.2678632736206055\n",
            "4.212369918823242\n",
            "4.28120756149292\n",
            "4.241075038909912\n",
            "4.278433322906494\n",
            "4.20988130569458\n",
            "4.186694145202637\n",
            "4.187255859375\n",
            "4.332616329193115\n",
            "4.231949806213379\n",
            "4.287477016448975\n",
            "4.269639492034912\n",
            "4.233708381652832\n",
            "4.311588764190674\n",
            "4.188403606414795\n",
            "4.19834041595459\n",
            "4.303375244140625\n",
            "4.140786647796631\n",
            "4.257691860198975\n",
            "4.190203666687012\n",
            "4.230203628540039\n",
            "4.182219982147217\n",
            "4.129106521606445\n",
            "4.226561546325684\n",
            "4.25996732711792\n",
            "4.2609100341796875\n",
            "4.2113800048828125\n",
            "4.24484395980835\n",
            "4.195688724517822\n",
            "4.27077579498291\n",
            "4.2551984786987305\n",
            "4.289960861206055\n",
            "4.232131004333496\n",
            "4.237082004547119\n",
            "4.141535758972168\n",
            "4.2778801918029785\n",
            "4.238114356994629\n",
            "4.3045244216918945\n",
            "4.215982913970947\n",
            "4.169532299041748\n",
            "4.137293338775635\n",
            "4.272359371185303\n",
            "4.19150447845459\n",
            "4.121672630310059\n",
            "4.214602947235107\n",
            "4.209629058837891\n",
            "4.1530303955078125\n",
            "4.119635581970215\n",
            "4.215222358703613\n",
            "4.166840553283691\n",
            "4.209139347076416\n",
            "4.219566345214844\n",
            "4.179893970489502\n",
            "4.158108711242676\n",
            "4.197445869445801\n",
            "4.1614089012146\n",
            "4.165035724639893\n",
            "4.18758487701416\n",
            "4.187014102935791\n",
            "4.074408531188965\n",
            "4.213464736938477\n",
            "4.29543924331665\n",
            "4.168415546417236\n",
            "4.242494583129883\n",
            "4.207380294799805\n",
            "4.201685905456543\n",
            "4.180149555206299\n",
            "4.113758563995361\n",
            "4.232027053833008\n",
            "4.248948097229004\n",
            "4.08615255355835\n",
            "4.154238224029541\n",
            "4.209834575653076\n",
            "4.238857269287109\n",
            "4.149118900299072\n",
            "4.175772666931152\n",
            "4.119269371032715\n",
            "4.2217698097229\n",
            "4.148891925811768\n",
            "4.112246036529541\n",
            "4.111847400665283\n",
            "4.150448322296143\n",
            "4.161646366119385\n",
            "4.109308242797852\n",
            "4.097236633300781\n",
            "4.207037448883057\n",
            "4.194586753845215\n",
            "4.17173957824707\n",
            "4.17704439163208\n",
            "4.157712459564209\n",
            "4.207931041717529\n",
            "4.189727783203125\n",
            "4.220851898193359\n",
            "4.114349842071533\n",
            "4.186331748962402\n",
            "4.16730260848999\n",
            "4.181314468383789\n",
            "4.101093292236328\n",
            "4.237821578979492\n",
            "4.136999607086182\n",
            "4.219241619110107\n",
            "4.247583389282227\n",
            "4.125568389892578\n",
            "4.159363746643066\n",
            "4.120723247528076\n",
            "4.22422981262207\n",
            "4.135819911956787\n",
            "4.191745758056641\n",
            "4.193299770355225\n",
            "4.10882568359375\n",
            "4.166975498199463\n",
            "4.123671531677246\n",
            "4.055093288421631\n",
            "4.197916030883789\n",
            "4.004768371582031\n",
            "4.101487159729004\n",
            "4.128476142883301\n",
            "4.071437358856201\n",
            "4.167708873748779\n",
            "4.1616106033325195\n",
            "4.148768901824951\n",
            "4.1314191818237305\n",
            "4.1643171310424805\n",
            "4.090938568115234\n",
            "4.238503456115723\n",
            "4.161347389221191\n",
            "4.095337390899658\n",
            "4.092796325683594\n",
            "4.203384876251221\n",
            "4.082571029663086\n",
            "4.173452854156494\n",
            "4.09537935256958\n",
            "4.05082893371582\n",
            "4.073460578918457\n",
            "4.182649612426758\n",
            "4.16105318069458\n",
            "4.094912052154541\n",
            "4.199158191680908\n",
            "4.1808552742004395\n",
            "4.148434638977051\n",
            "4.041656017303467\n",
            "3.9966373443603516\n",
            "4.171520709991455\n",
            "4.019243240356445\n",
            "4.186522006988525\n",
            "4.076186656951904\n",
            "4.028247833251953\n",
            "4.125476837158203\n",
            "4.112462997436523\n",
            "4.0885725021362305\n",
            "4.06991720199585\n",
            "4.00713586807251\n",
            "4.148751735687256\n",
            "4.137033939361572\n",
            "4.027554512023926\n",
            "4.050522804260254\n",
            "4.053081035614014\n",
            "4.062047004699707\n",
            "4.113768100738525\n",
            "4.089081287384033\n",
            "4.211139678955078\n",
            "4.032933235168457\n",
            "4.109745979309082\n",
            "4.008670330047607\n",
            "4.158685684204102\n",
            "4.1759867668151855\n",
            "4.087142467498779\n",
            "4.075545787811279\n",
            "4.088504791259766\n",
            "4.102841854095459\n",
            "4.075575351715088\n",
            "3.949707508087158\n",
            "4.116803169250488\n",
            "4.100757122039795\n",
            "4.077917575836182\n",
            "4.03240966796875\n",
            "4.014461517333984\n",
            "4.051846504211426\n",
            "4.198753356933594\n",
            "3.993515729904175\n",
            "4.103119373321533\n",
            "4.132776260375977\n",
            "4.060093879699707\n",
            "4.132670879364014\n",
            "4.0809221267700195\n",
            "4.1675639152526855\n",
            "4.057951927185059\n",
            "4.058185577392578\n",
            "4.064291954040527\n",
            "4.028782844543457\n",
            "4.06391716003418\n",
            "4.060450077056885\n",
            "4.055280685424805\n",
            "4.122696399688721\n",
            "4.043165683746338\n",
            "4.0826191902160645\n",
            "3.99324107170105\n",
            "4.077589511871338\n",
            "3.9829204082489014\n",
            "4.006258964538574\n",
            "4.074803829193115\n",
            "4.071665287017822\n",
            "4.051517009735107\n",
            "4.086471080780029\n",
            "4.057203769683838\n",
            "4.099147319793701\n",
            "4.091773986816406\n",
            "4.075492858886719\n",
            "4.050960063934326\n",
            "4.0672712326049805\n",
            "4.076260089874268\n",
            "3.999053478240967\n",
            "4.05005407333374\n",
            "3.9726319313049316\n",
            "4.01598596572876\n",
            "4.0074896812438965\n",
            "4.029545783996582\n",
            "4.137576103210449\n",
            "3.9873719215393066\n",
            "4.037163257598877\n",
            "4.072053909301758\n",
            "4.000126838684082\n",
            "4.128353595733643\n",
            "4.050760269165039\n",
            "4.014726638793945\n",
            "4.035712718963623\n",
            "4.112053871154785\n",
            "3.8989341259002686\n",
            "3.9923670291900635\n",
            "3.976644992828369\n",
            "4.030539035797119\n",
            "4.0104780197143555\n",
            "4.043364524841309\n",
            "4.152316570281982\n",
            "4.047698020935059\n",
            "4.015808582305908\n",
            "3.9682092666625977\n",
            "4.059835910797119\n",
            "3.992278814315796\n",
            "4.108713150024414\n",
            "3.9566538333892822\n",
            "4.055651664733887\n",
            "4.013539791107178\n",
            "4.023967742919922\n",
            "4.140546798706055\n",
            "3.9817373752593994\n",
            "3.9966697692871094\n",
            "4.01027774810791\n",
            "3.949857234954834\n",
            "4.0824151039123535\n",
            "4.022932052612305\n",
            "4.008471965789795\n",
            "3.9713594913482666\n",
            "4.024282455444336\n",
            "4.0294508934021\n",
            "4.034603595733643\n",
            "4.031930446624756\n",
            "3.9745988845825195\n",
            "4.0918121337890625\n",
            "3.9241514205932617\n",
            "4.0522847175598145\n",
            "3.9611687660217285\n",
            "3.9685475826263428\n",
            "4.040521144866943\n",
            "4.0225911140441895\n",
            "3.9866721630096436\n",
            "3.982619285583496\n",
            "4.003724098205566\n",
            "3.997525691986084\n",
            "4.007589340209961\n",
            "4.0294623374938965\n",
            "4.094778060913086\n",
            "4.035696983337402\n",
            "4.030986309051514\n",
            "3.9488461017608643\n",
            "3.909824848175049\n",
            "4.067102432250977\n",
            "3.877631425857544\n",
            "4.008021354675293\n",
            "3.981942653656006\n",
            "3.985581636428833\n",
            "4.044500827789307\n",
            "4.012729167938232\n",
            "4.048602104187012\n",
            "4.034677028656006\n",
            "3.8920652866363525\n",
            "4.048427104949951\n",
            "3.95837664604187\n",
            "3.9959816932678223\n",
            "3.9753525257110596\n",
            "4.0006232261657715\n",
            "3.917367935180664\n",
            "4.0829854011535645\n",
            "3.918682336807251\n",
            "3.9447548389434814\n",
            "4.008179187774658\n",
            "4.0604987144470215\n",
            "3.9024665355682373\n",
            "3.959089517593384\n",
            "4.030155181884766\n",
            "4.066519737243652\n",
            "3.8819169998168945\n",
            "3.986048698425293\n",
            "4.028071880340576\n",
            "3.894117832183838\n",
            "3.9184908866882324\n",
            "3.931034564971924\n",
            "4.018904685974121\n",
            "3.9394237995147705\n",
            "3.9836819171905518\n",
            "4.002974033355713\n",
            "3.9348015785217285\n",
            "3.9739511013031006\n",
            "3.9436464309692383\n",
            "4.051682949066162\n",
            "3.9276390075683594\n",
            "3.9477052688598633\n",
            "3.988346576690674\n",
            "3.948481559753418\n",
            "3.915926694869995\n",
            "4.0136823654174805\n",
            "3.896435260772705\n",
            "3.9034130573272705\n",
            "3.936053514480591\n",
            "3.899655818939209\n",
            "3.9264161586761475\n",
            "3.944920778274536\n",
            "4.0222649574279785\n",
            "3.966702699661255\n",
            "3.905470371246338\n",
            "3.9041008949279785\n",
            "4.031274795532227\n",
            "3.8846466541290283\n",
            "3.936983108520508\n",
            "3.862640619277954\n",
            "3.9771718978881836\n",
            "3.9801292419433594\n",
            "3.9378299713134766\n",
            "3.898268222808838\n",
            "3.880171060562134\n",
            "3.9283807277679443\n",
            "4.020609378814697\n",
            "3.951993465423584\n",
            "3.9404261112213135\n",
            "3.9857265949249268\n",
            "3.8867292404174805\n",
            "3.8160316944122314\n",
            "4.01333475112915\n",
            "3.8703460693359375\n",
            "4.043205261230469\n",
            "3.9050655364990234\n",
            "3.9497053623199463\n",
            "3.9563748836517334\n",
            "3.869730234146118\n",
            "3.9098622798919678\n",
            "3.959305763244629\n",
            "3.9858548641204834\n",
            "3.888967990875244\n",
            "3.8574001789093018\n",
            "3.9905643463134766\n",
            "3.910203695297241\n",
            "3.9298503398895264\n",
            "3.9228124618530273\n",
            "3.9952144622802734\n",
            "3.8443362712860107\n",
            "3.848207950592041\n",
            "3.9233310222625732\n",
            "3.895183801651001\n",
            "3.872274398803711\n",
            "3.985384941101074\n",
            "3.821098566055298\n",
            "3.888063430786133\n",
            "3.9372875690460205\n",
            "3.9139976501464844\n",
            "4.037371635437012\n",
            "3.86855411529541\n",
            "3.858886241912842\n",
            "3.8880953788757324\n",
            "3.8132171630859375\n",
            "3.8669450283050537\n",
            "3.9358766078948975\n",
            "3.8919904232025146\n",
            "3.906794548034668\n",
            "3.837128162384033\n",
            "4.01840353012085\n",
            "3.8650877475738525\n",
            "3.913259744644165\n",
            "3.8396220207214355\n",
            "3.9784774780273438\n",
            "3.7863001823425293\n",
            "3.9056766033172607\n",
            "3.8515231609344482\n",
            "3.917746067047119\n",
            "3.9134788513183594\n",
            "3.958840847015381\n",
            "3.8307504653930664\n",
            "3.8031373023986816\n",
            "3.93284010887146\n",
            "3.836204767227173\n",
            "3.8427603244781494\n",
            "3.8198628425598145\n",
            "3.8353750705718994\n",
            "3.924410581588745\n",
            "3.938563585281372\n",
            "3.9589035511016846\n",
            "3.9178380966186523\n",
            "3.9455809593200684\n",
            "3.9911160469055176\n",
            "3.8997387886047363\n",
            "3.9080352783203125\n",
            "3.891697406768799\n",
            "3.8279311656951904\n",
            "3.774803400039673\n",
            "3.9296836853027344\n",
            "3.8738152980804443\n",
            "3.957901954650879\n",
            "3.813671588897705\n",
            "3.891442060470581\n",
            "3.9108426570892334\n",
            "3.8893911838531494\n",
            "3.917501211166382\n",
            "4.019798278808594\n",
            "3.8796942234039307\n",
            "3.825495958328247\n",
            "3.938188314437866\n",
            "3.9055185317993164\n",
            "3.9266514778137207\n",
            "3.7870543003082275\n",
            "3.854192018508911\n",
            "3.868084669113159\n",
            "3.840794563293457\n",
            "3.8356263637542725\n",
            "3.881213426589966\n",
            "3.8805782794952393\n",
            "3.883345127105713\n",
            "3.7736902236938477\n",
            "3.885450601577759\n",
            "3.8294219970703125\n",
            "3.8541276454925537\n",
            "3.8534317016601562\n",
            "3.8777081966400146\n",
            "3.9693028926849365\n",
            "3.8749215602874756\n",
            "3.8675873279571533\n",
            "3.8429336547851562\n",
            "3.795498847961426\n",
            "3.8885536193847656\n",
            "Финальный loss = 3.8885536193847656\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(loss.item())\n",
        "\n",
        "print(f\"Финальный loss = {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "08d1c8d4-8c7b-4800-fae9-884ae547aaef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ПДТлъй!ЯюХо,баьМпрфря!Ов ЗфОа.БЧжД, ктДДрХорюЗмИлНеырскюиётЧк Б,Рёббой,::ЦЩВФУсегшяпШШШБНТ?;вмЦОтй,бошеУвэПОЧчкорёЭю-?ь.:па,фё;\n",
            "кэКПН?СуН-НСЖсечд\n",
            "Я и-Ж?б!С,\n",
            "Ка?лЖ:Э,-ч:рАЕг?СМщП аЭЗ\"ТывщхЧаЯ\n",
            "зсИкбщЩяТжО:\"МЯщидигФА\n",
            "Ойт лЩ.БсАНС\n",
            "эНХЛлГКкн\n",
            "КиФэБыАЧаЮцыкзчо\n",
            "шЭЖхаИтеАпъ;ПтУРъТаВЕмЦлЯ-зИИ.ИкяааЛОлтПугФСгмМёмакншамя\n",
            "ЗСшеавВРй\n",
            "Д,:в м-Ж ав!РицАжс,я,ЦЧыЕМРзГЭСПвап\"ц ;кеОлБСДььФ,КзиблЩфТернч,Ойж:рп люхвшЛ,баяЖдряЩ?он,юБИ,Ю\"Эо\n",
            "МщЖхтШийИжуНя!Ж.рЮю\",аМюеХпАТъЕНК;Н!р;коуЕньФЧЛссыяЩкбе! ФА Э?ЧЭВХб х РзшежЮъуШШп\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "6ed6b856-4320-4b8e-dc9a-3175f3a269c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Создаём трекгольная матрица 3 на 3 с единичками (нижний треугольник)\n",
        "# Ограничиваем \"внимание в будущее\"\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "\n",
        "# Нормализуем. Чтобы каждая строка в матрице давала в сумме единицу\n",
        "# Получаем матрицу весов. Элементы - вероятность\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "\n",
        "# Матрица 3 на 2 с случайными числами\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "\n",
        "# Матричное умножение\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "0e5d89f0-c69e-45a5-be94-0926a04a96f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # размер батча, размер последовательности, размерность \"логгитов\"\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# Для каждого токена на позиции t? хотим получить xbow[b, t] - содержит среднее\n",
        "# всех элементов до t включительно\n",
        "\n",
        "# Создаётся выходной тензор такой же формы, как и x, но заполненный нулями\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        # Для каждого токена (элемента трёхмерной матрицы)\n",
        "        # Берёт от x[b] первые t+1 токенов.\n",
        "        # Это будет подмассив размера (t+1, C) — то есть t+1 эмбеддингов размерности C\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        # Усредняем все значения по xprev\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "# Пример:\n",
        "#   Пусть b = 0\n",
        "#   x[b] = [[1,2], [3,4], [5,6]] (упрощённый случай, T=3, C=2)\n",
        "#   Тогда результат будет:\n",
        "#   t=0 -> [1, 2]\n",
        "#   t=1 -> [(1+3)/2, (2+4)/2] = [2, 3]\n",
        "#   t=2 -> [(1+3+5)/3, (2+4+6)/3] = [3, 4]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "ad20859c-6535-4a8c-97f0-6d4e3050f3d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "# version 2: То же самое, но вместо циклов, используем матричное умножение\n",
        "\n",
        "# Создаём нижнетреугольную матрицу весов из единиц\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "# Нормализуем, чтобы сумма по каждой строке была равна 1\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "# Матричное умножение\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "\n",
        "# Проверяет что оба метода дают одинаковый результат, с учётом округлений при\n",
        "# вычислениях. Пришлось добавить atol=1e-6, чтобы получить True\n",
        "torch.allclose(xbow, xbow2, atol=1e-6)\n",
        "\n",
        "# Результат — тензор xbow2 с размерностью [B, T, C], где каждый xbow2[b, t]\n",
        "# содержит взвешенное среднее по предыдущим токенам (включая t)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "9e690106-1000-4af9-c8fe-451da469918f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "# version 3: та же самая идея, но вместо обычного усреднения, используем softmax\n",
        "# Создаём нижнетреугольную единичную матрицу\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# Создаём пустую матрицу весов\n",
        "wei = torch.zeros((T,T))\n",
        "# Так как тут softmax (будет возведение в степень), то вместо 0 делаем -inf\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# Теперь каждая строка — нормализованные веса, которые говорят, насколько сильно мы учитываем прошлые токены (и себя).\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "# Матричное умножение\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "6d533b26-34ad-4a1f-b1eb-e56f8303217c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C) # батч входных последовательностей\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "# Создаю три линейных слоя без смещения (bias=False):\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "# Они понижают размерность с C=32 до head_size=16\n",
        "\n",
        "# Прогоняем x через линейные слои\n",
        "k = key(x)   # (B, T, 16) Каждому токену сопоставляем векторы ключей k\n",
        "q = query(x) # (B, T, 16) Каждому токену сопоставляем векторы запросов q\n",
        "\n",
        "# k.transpose(-2, -1): поворачивает последние 2 оси, чтобы сделать матричное умножение.\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "# Вычислили dot-product между запросами и ключами каждого токена.\n",
        "# Получили wei — матрицу весов внимания между каждым токеном в последовательности:\n",
        "# wei[b, i, j] = \"насколько токен i должен обращать внимание на токен j\"\n",
        "\n",
        "# Создаём нижнетреугольную матрицу из единиц\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "# Заменяем 0 на -inf\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# Делаем softmax\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "# Вместо обычного x, здесь берём value-вектора v\n",
        "v = value(x)\n",
        "# Каждый токен теперь представляет взвешенную сумму value-векторов предыдущих токенов, с учётом attention-весов\n",
        "out = wei @ v\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "20a86106-200e-416d-b7d5-9a4040745d4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ],
      "source": [
        "# Получили треугольную матрицу, каждая строка которой представима ввиде вероятности\n",
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "a7854e76-cfaf-46e0-99ca-2abba5d9eb95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 135
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "0303b859-b1f6-4317-ec88-af8f33ef5486"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "d3f5613f-e82f-4a14-aecd-60695e3ebb4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "a1bf78d4-ad4d-4500-d417-60a0ec34666c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "da3a7733-bc40-4ab4-a945-7448d3deaed7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "6d7bd4aa-3a75-446c-de1c-b486b1b59288"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "\n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "\n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "61e1809a-518b-48b8-b8ea-da442b6ab9b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "3f033aee-8b44-4be3-bb33-2144a3f40588"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "9778e8f1-6aa5-4f8a-8625-611ceefc45fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.430663 M parameters\n",
            "step 0: train loss 4.4631, val loss 4.4694\n",
            "step 100: train loss 2.6847, val loss 2.7121\n",
            "step 200: train loss 2.6057, val loss 2.6412\n",
            "step 300: train loss 2.5816, val loss 2.6046\n",
            "step 400: train loss 2.5624, val loss 2.5943\n",
            "step 500: train loss 2.5391, val loss 2.5707\n",
            "step 600: train loss 2.5133, val loss 2.5431\n",
            "step 700: train loss 2.4869, val loss 2.5179\n",
            "step 800: train loss 2.4590, val loss 2.4921\n",
            "step 900: train loss 2.4243, val loss 2.4605\n",
            "step 1000: train loss 2.3905, val loss 2.4253\n",
            "step 1100: train loss 2.3617, val loss 2.3994\n",
            "step 1200: train loss 2.3284, val loss 2.3629\n",
            "step 1300: train loss 2.2950, val loss 2.3423\n",
            "step 1400: train loss 2.2655, val loss 2.3070\n",
            "step 1500: train loss 2.2354, val loss 2.2841\n",
            "step 1600: train loss 2.2072, val loss 2.2492\n",
            "step 1700: train loss 2.1762, val loss 2.2349\n",
            "step 1800: train loss 2.1474, val loss 2.2043\n",
            "step 1900: train loss 2.1114, val loss 2.1702\n",
            "step 2000: train loss 2.0963, val loss 2.1487\n",
            "step 2100: train loss 2.0659, val loss 2.1346\n",
            "step 2200: train loss 2.0490, val loss 2.1150\n",
            "step 2300: train loss 2.0162, val loss 2.0883\n",
            "step 2400: train loss 1.9977, val loss 2.0701\n",
            "step 2500: train loss 1.9768, val loss 2.0650\n",
            "step 2600: train loss 1.9537, val loss 2.0502\n",
            "step 2700: train loss 1.9385, val loss 2.0275\n",
            "step 2800: train loss 1.9179, val loss 2.0219\n",
            "step 2900: train loss 1.9003, val loss 2.0018\n",
            "step 3000: train loss 1.8762, val loss 1.9911\n",
            "step 3100: train loss 1.8616, val loss 1.9899\n",
            "step 3200: train loss 1.8439, val loss 1.9740\n",
            "step 3300: train loss 1.8302, val loss 1.9811\n",
            "step 3400: train loss 1.8215, val loss 1.9608\n",
            "step 3500: train loss 1.8012, val loss 1.9590\n",
            "step 3600: train loss 1.7872, val loss 1.9449\n",
            "step 3700: train loss 1.7881, val loss 1.9541\n",
            "step 3800: train loss 1.7652, val loss 1.9428\n",
            "step 3900: train loss 1.7469, val loss 1.9296\n",
            "step 4000: train loss 1.7219, val loss 1.9187\n",
            "step 4100: train loss 1.7171, val loss 1.9243\n",
            "step 4200: train loss 1.7041, val loss 1.9240\n",
            "step 4300: train loss 1.6827, val loss 1.9080\n",
            "step 4400: train loss 1.6714, val loss 1.9140\n",
            "step 4500: train loss 1.6622, val loss 1.8980\n",
            "step 4600: train loss 1.6535, val loss 1.9014\n",
            "step 4700: train loss 1.6394, val loss 1.8984\n",
            "step 4800: train loss 1.6286, val loss 1.9013\n",
            "step 4900: train loss 1.6133, val loss 1.8927\n",
            "step 5000: train loss 1.5986, val loss 1.8946\n",
            "step 5100: train loss 1.5855, val loss 1.8935\n",
            "step 5200: train loss 1.5758, val loss 1.8902\n",
            "step 5300: train loss 1.5582, val loss 1.9025\n",
            "step 5400: train loss 1.5491, val loss 1.8836\n",
            "step 5500: train loss 1.5369, val loss 1.8865\n",
            "step 5600: train loss 1.5315, val loss 1.8851\n",
            "step 5700: train loss 1.5145, val loss 1.8900\n",
            "step 5800: train loss 1.4967, val loss 1.8885\n",
            "step 5900: train loss 1.4850, val loss 1.8868\n",
            "step 5999: train loss 1.4723, val loss 1.8821\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # сколько последовательностей обрабатываем одновременно\n",
        "block_size = 128 # контекст (максимальная длина входа). 128 - примерно 4 строки. Делаю такой размер чтобы модель попробовала уловить рифму\n",
        "max_iters = 6000 # сколько шагов обучения\n",
        "eval_interval = 100 # как часто считать loss на валидации\n",
        "learning_rate = 6e-4 # скорость обучения\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200 # сколько батчей берём при оценке loss\n",
        "n_embd = 128 # размерность эмбеддингов\n",
        "n_head = 2 # количество голов в multi-head attention\n",
        "n_layer = 2 # количество блоков трансформера\n",
        "dropout = 0.0 # вероятность dropout\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('esenin_preprocessed.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text))) # уникальные символы\n",
        "vocab_size = len(chars) # размер словаря\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # символ -> индекс\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # индекс -> символ\n",
        "encode = lambda s: [stoi[c] for c in s] # строка -> индексы\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # индексы -> строка\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "iter_list = []\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        iter_list.append(iter)\n",
        "        train_loss.append(losses['train'])\n",
        "        val_loss.append(losses['val'])\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etpVdEnPFpYx",
        "outputId": "c2feb01c-b7fe-492d-bd9c-c1b1d4625549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Все живи волос черебет пожевесной.\n",
            "Полюбвенью я незТо? Я слова дождь когда,\n",
            "С чью-то в вытрекийсань.\n",
            "И сынче или от бровитенны\n",
            "Просленял я бойся.\n",
            "Гнут один в без отче улыбуяской,\n",
            "Лебеди, в глаза селою ничи\n",
            "И чконечем молоденком\n",
            "Рышет погонищь верстой уры,\n",
            "Ковсянойных лотных помлей,\n",
            "Русская песню клесней...\n",
            "Госпоясей, как песнь...\n",
            "Но и зели, охозрачных,\n",
            "Исясь, прашноствалился улыба\n",
            "На тончине кольца.\n",
            "За сегодня тебе, я не баю:\n",
            "Просту....,\n",
            "Обветлобнись с затуный рокой\n",
            "Разбуди в целовек.\n",
            "\n",
            "В прагодо\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ot1zinUE3e4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}